{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08e91cdb-4693-4632-b7aa-f37eec027131",
   "metadata": {},
   "source": [
    "## Working with Transformers in the HuggingFace Ecosystem\n",
    "\n",
    "In this laboratory exercise we will learn how to work with the HuggingFace ecosystem to adapt models to new tasks. As you will see, much of what is required is *investigation* into the inner-workings of the HuggingFace abstractions. With a little work, a little trial-and-error, it is fairly easy to get a working adaptation pipeline up and running."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e556105-269f-43e3-8933-227269afb9ea",
   "metadata": {},
   "source": [
    "### Exercise 1: Sentiment Analysis (warm up)\n",
    "\n",
    "In this first exercise we will start from a pre-trained BERT transformer and build up a model able to perform text sentiment analysis. Transformers are complex beasts, so we will build up our pipeline in several explorative and incremental steps.\n",
    "\n",
    "#### Exercise 1.1: Dataset Splits and Pre-trained model\n",
    "There are a many sentiment analysis datasets, but we will use one of the smallest ones available: the [Cornell Rotten Tomatoes movie review dataset](cornell-movie-review-data/rotten_tomatoes), which consists of 5,331 positive and 5,331 negative processed sentences from the Rotten Tomatoes movie reviews.\n",
    "\n",
    "**Your first task**: Load the dataset and figure out what splits are available and how to get them. Spend some time exploring the dataset to see how it is organized. Note that we will be using the [HuggingFace Datasets](https://huggingface.co/docs/datasets/en/index) library for downloading, accessing, splitting, and batching data for training and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e514482a",
   "metadata": {},
   "source": [
    "## **Import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04bf3cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "#import models\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import pipeline\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import DataCollatorWithPadding, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "# Import PyTorch\n",
    "import torch\n",
    "\n",
    "#import Datasets\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f34a8c8",
   "metadata": {},
   "source": [
    "\n",
    "### ***Further settings***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dfd1508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "torch.manual_seed(808)\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf8a04f",
   "metadata": {},
   "source": [
    "### **Load the dataset and split checking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15819af4-e850-412b-ab67-61b9b98e3a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available splits: dict_keys(['train', 'validation', 'test'])\n",
      "Labels in the dataset: [0 1]\n",
      "train: 8530 examples\n",
      "validation: 1066 examples\n",
      "test: 1066 examples\n",
      "\n",
      "Examples from the training set:\n",
      "{'text': 'the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .', 'label': 1}\n",
      "{'text': \"emerges as something rare , an issue movie that's so honest and keenly observed that it doesn't feel like one .\", 'label': 1}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the dataset from HuggingFace\n",
    "dataset = load_dataset(\"rotten_tomatoes\")\n",
    "dataset_id = \"cornell-movie-review-data/rotten_tomatoes\"\n",
    "ds_train = load_dataset(dataset_id, split=\"train\")\n",
    "ds_test = load_dataset(dataset_id, split=\"test\")\n",
    "ds_validation = load_dataset(dataset_id, split=\"validation\")\n",
    "\n",
    "# available splits\n",
    "print(\"Available splits:\", dataset.keys())\n",
    "print(f\"Labels in the dataset: {np.unique(ds_train['label'])}\")\n",
    "\n",
    "\n",
    "for split_name, split_data in dataset.items():\n",
    "    print(f\"{split_name}: {len(split_data)} examples\")\n",
    "\n",
    "    # some rows from the training set\n",
    "print(\"\\nExamples from the training set:\")\n",
    "print(dataset[\"train\"][0])   # first example\n",
    "print(dataset[\"train\"][4])   # second example\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27f50a0-0af4-4bdd-9a60-bab26f126c14",
   "metadata": {},
   "source": [
    "#### Exercise 1.2: A Pre-trained BERT and Tokenizer\n",
    "\n",
    "The model we will use is a *very* small BERT transformer called [Distilbert](https://huggingface.co/distilbert/distilbert-base-uncased) this model was trained (using self-supervised learning) on the same corpus as BERT but using the full BERT base model as a *teacher*.\n",
    "\n",
    "**Your next task**: Load the Distilbert model and corresponding tokenizer. Use the tokenizer on a few samples from the dataset and pass the tokens through the model to see what outputs are provided. I suggest you use the [`AutoModel`](https://huggingface.co/transformers/v3.0.2/model_doc/auto.html) class (and the `from_pretrained()` method) to load the model and `AutoTokenizer` to load the tokenizer)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccc38f5",
   "metadata": {},
   "source": [
    "### ***DistilBERT***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3eefc651-8551-44c2-8340-37d64660fc58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text: steers turns in a snappy screenplay that curls at the edges ; it's so clever you want to hate it . but he somehow pulls it off .\n",
      "\n",
      "model output: odict_keys(['last_hidden_state'])\n",
      "\n",
      "Shape of last hidden states: torch.Size([1, 34, 768])\n",
      "Last hidden states for the sample:\n",
      "tensor([[[-0.1363, -0.0400,  0.0322,  ..., -0.1565,  0.5009,  0.2881],\n",
      "         [ 0.2580,  0.4807,  0.1554,  ...,  0.0087,  0.6204, -0.1539],\n",
      "         [-0.3299, -0.2383,  0.1396,  ..., -0.1244,  0.6183, -0.1483],\n",
      "         ...,\n",
      "         [ 0.0929, -0.4927,  0.1931,  ...,  0.0601,  0.1228, -0.1329],\n",
      "         [ 0.6391,  0.1454, -0.3272,  ..., -0.0744, -0.1469, -0.4849],\n",
      "         [-0.2033,  0.6055,  0.6094,  ..., -0.0866,  0.5897,  0.0293]]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the pre-trained DistilBERT tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Take a single sample from the training set\n",
    "sample_text = ds_train[8][\"text\"]\n",
    "print(\"Sample text:\", sample_text)\n",
    "\n",
    "# Tokenize the sample\n",
    "inputs = tokenizer(sample_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "# Forward pass without gradient calculation (faster, less memory)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "print(f\"\\nmodel output: {outputs.keys()}\")\n",
    "\n",
    "# Extract the last hidden states\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "\n",
    "print(\"\\nShape of last hidden states:\", last_hidden_states.shape)\n",
    "print(\"Last hidden states for the sample:\")\n",
    "print(last_hidden_states)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0f6fd4-b80b-466d-b4ff-9aac0492c062",
   "metadata": {},
   "source": [
    "#### Exercise 1.3: A Stable Baseline\n",
    "\n",
    "In this exercise I want you to:\n",
    "1. Use Distilbert as a *feature extractor* to extract representations of the text strings from the dataset splits;\n",
    "2. Train a classifier (your choice, by an SVM from Scikit-learn is an easy choice).\n",
    "3. Evaluate performance on the validation and test splits.\n",
    "\n",
    "These results are our *stable baseline* -- the **starting** point on which we will (hopefully) improve in the next exercise.\n",
    "\n",
    "**Hint**: There are a number of ways to implement the feature extractor, but probably the best is to use a [feature extraction `pipeline`](https://huggingface.co/tasks/feature-extraction). You will need to interpret the output of the pipeline and extract only the `[CLS]` token from the *last* transformer layer. *How can you figure out which output that is?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e313e616",
   "metadata": {},
   "source": [
    "### **Feature extraction with Distilbert and SVM (Linear) Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2068b0a2",
   "metadata": {},
   "source": [
    "### *Checking CLS token position*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ed16917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['[CLS]', 'welcome', 'to', 'florence', '!', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "text = \" welcome to Florence!\"\n",
    "encoding = tokenizer(text, return_tensors=\"pt\")\n",
    "tokens = tokenizer.convert_ids_to_tokens(encoding['input_ids'][0])\n",
    "\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ebb961ab-aa24-4d5c-86a1-c9b4237b40fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.8189\n",
      "Test accuracy: 0.8068\n"
     ]
    }
   ],
   "source": [
    "# Initialize feature-extraction pipeline with DistilBERT\n",
    "feature_extractor = pipeline(\"feature-extraction\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "def extract_cls_embeddings(texts):\n",
    "    # Extract CLS token embeddings for a batch of texts\n",
    "    features = feature_extractor(texts, truncation=True, padding=True)\n",
    "    cls_embeddings = []\n",
    "    for f in features:\n",
    "        # f has shape (1, seq_len, 768)\n",
    "        # Take the first token embedding (CLS token)\n",
    "        cls_embedding = f[0][0]  \n",
    "        cls_embeddings.append(cls_embedding)\n",
    "    return np.array(cls_embeddings) \n",
    "\n",
    "# Extract features for train, validation and test splits\n",
    "X_train = extract_cls_embeddings(list(ds_train[\"text\"]))\n",
    "y_train = np.array(ds_train[\"label\"])\n",
    "\n",
    "X_val = extract_cls_embeddings(list(ds_validation[\"text\"]))\n",
    "y_val = np.array(ds_validation[\"label\"])\n",
    "\n",
    "X_test = extract_cls_embeddings(list(ds_test[\"text\"]))\n",
    "y_test = np.array(ds_test[\"label\"])\n",
    "\n",
    "# Train an SVM classifier\n",
    "clf = SVC(kernel='linear')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on validation and test sets\n",
    "val_preds = clf.predict(X_val)\n",
    "test_preds = clf.predict(X_test)\n",
    "\n",
    "val_acc = accuracy_score(y_val, val_preds)\n",
    "test_acc = accuracy_score(y_test, test_preds)\n",
    "\n",
    "print(f\"Validation accuracy: {val_acc:.4f}\")\n",
    "print(f\"Test accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37141d1b-935b-425c-804c-b9b487853791",
   "metadata": {},
   "source": [
    "-----\n",
    "### Exercise 2: Fine-tuning Distilbert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a53f64-0238-42f2-bc20-86e51c77d2e5",
   "metadata": {},
   "source": [
    "In this exercise we will fine-tune the Distilbert model to (hopefully) improve sentiment analysis performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3392b1ed-597b-4a92-90fc-10eb11eac515",
   "metadata": {},
   "source": [
    "#### Exercise 2.1: Token Preprocessing\n",
    "\n",
    "The first thing we need to do is *tokenize* our dataset splits. Our current datasets return a dictionary with *strings*, but we want *input token ids* (i.e. the output of the tokenizer). This is easy enough to do my hand, but the HugginFace `Dataset` class provides convenient, efficient, and *lazy* methods. See the documentation for [`Dataset.map`](https://huggingface.co/docs/datasets/v3.5.0/en/package_reference/main_classes#datasets.Dataset.map).\n",
    "\n",
    "**Tip**: Verify that your new datasets are returning for every element: `text`, `label`, `intput_ids`, and `attention_mask`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97be4aa0",
   "metadata": {},
   "source": [
    "### *Token Preprocessing*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e6e2a95-5b08-4f81-b824-59a0fb3404e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text', 'label', 'input_ids', 'attention_mask']\n",
      "\n",
      "{'label': tensor(1), 'input_ids': tensor([  101,  1996,  2600,  2003, 16036,  2000,  2022,  1996,  7398,  2301,\n",
      "         1005,  1055,  2047,  1000, 16608,  1000,  1998,  2008,  2002,  1005,\n",
      "         1055,  2183,  2000,  2191,  1037, 17624,  2130,  3618,  2084,  7779,\n",
      "        29058,  8625, 13327,  1010,  3744,  1011, 18856, 19513,  3158,  5477,\n",
      "         4168,  2030,  7112, 16562,  2140,  1012,   102,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])}\n"
     ]
    }
   ],
   "source": [
    "# Define tokenization function\n",
    "def tokenize_function(examples):\n",
    "\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True)\n",
    "\n",
    "# Apply tokenization on all splits with batching\n",
    "tokenized_train = ds_train.map(tokenize_function, batched=True)\n",
    "tokenized_val = ds_validation.map(tokenize_function, batched=True)\n",
    "tokenized_test = ds_test.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set the format for PyTorch\n",
    "tokenized_train.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "tokenized_val.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "tokenized_test.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "print(tokenized_train.column_names)\n",
    "# Example: check a tokenized batch\n",
    "print(f\"\\n{tokenized_train[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a80de2-83c9-4c12-af4e-34babe23ffd1",
   "metadata": {},
   "source": [
    "#### Exercise 2.2: Setting up the Model to be Fine-tuned\n",
    "\n",
    "In this exercise we need to prepare the base Distilbert model for fine-tuning for a *sequence classification task*. This means, at the very least, appending a new, randomly-initialized classification head connected to the `[CLS]` token of the last transformer layer. Luckily, HuggingFace already provides an `AutoModel` for just this type of instantiation: [`AutoModelForSequenceClassification`](https://huggingface.co/transformers/v3.0.2/model_doc/auto.html#automodelforsequenceclassification). You will want you instantiate one of these for fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2f67f7",
   "metadata": {},
   "source": [
    "### **Classification model defining**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6327d73-3b71-478a-9932-bb062a650c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Number of output classes for classification\n",
    "num_labels = 2  \n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=num_labels\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d109f7bd-955f-4fc4-bc3e-75bd99a17adf",
   "metadata": {},
   "source": [
    "#### Exercise 2.3: Fine-tuning Distilbert\n",
    "\n",
    "Finally. In this exercise you should use a HuggingFace [`Trainer`](https://huggingface.co/docs/transformers/main/en/trainer) to fine-tune your model on the Rotten Tomatoes training split. Setting up the trainer will involve (at least):\n",
    "\n",
    "\n",
    "1. Instantiating a [`DataCollatorWithPadding`](https://huggingface.co/docs/transformers/en/main_classes/data_collator) object which is what *actually* does your batch construction (by padding all sequences to the same length).\n",
    "2. Writing an *evaluation function* that will measure the classification accuracy. This function takes a single argument which is a tuple containing `(logits, labels)` which you should use to compute classification accuracy (and maybe other metrics like F1 score, precision, recall) and return a `dict` with these metrics.  \n",
    "3. Instantiating a [`TrainingArguments`](https://huggingface.co/docs/transformers/v4.51.1/en/main_classes/trainer#transformers.TrainingArguments) object using some reasonable defaults.\n",
    "4. Instantiating a `Trainer` object using your train and validation splits, you data collator, and function to compute performance metrics.\n",
    "5. Calling `trainer.train()`, waiting, waiting some more, and then calling `trainer.evaluate()` to see how it did.\n",
    "\n",
    "**Tip**: When prototyping this laboratory I discovered the HuggingFace [Evaluate library](https://huggingface.co/docs/evaluate/en/index) which provides evaluation metrics. However I found it to have insufferable layers of abstraction and getting actual metrics computed. I suggest just using the Scikit-learn metrics..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274781a2",
   "metadata": {},
   "source": [
    "## ***Fine-tuning and evaluation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab17178d-5028-47e3-af97-953e8de5aae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: emile-agbedanu (emile-agbedanu-none) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.2s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\UNIFI\\LM Bio UniFi\\II ANNO\\SECONDO SEMESTRE\\APPLI OF MACHINE LEARNING\\AML_Labs\\Exercise_3\\wandb\\run-20250814_001953-1ymbi8c7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/emile-agbedanu-none/Transformers/runs/1ymbi8c7' target=\"_blank\">DistilBERT_FineTuned</a></strong> to <a href='https://wandb.ai/emile-agbedanu-none/Transformers' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/emile-agbedanu-none/Transformers' target=\"_blank\">https://wandb.ai/emile-agbedanu-none/Transformers</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/emile-agbedanu-none/Transformers/runs/1ymbi8c7' target=\"_blank\">https://wandb.ai/emile-agbedanu-none/Transformers/runs/1ymbi8c7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emile\\AppData\\Local\\Temp\\ipykernel_18236\\3231484922.py:51: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='1602' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  63/1602 02:35 < 1:05:10, 0.39 it/s, Epoch 0.12/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.614600</td>\n",
       "      <td>0.479431</td>\n",
       "      <td>0.786116</td>\n",
       "      <td>0.788657</td>\n",
       "      <td>0.786116</td>\n",
       "      <td>0.785645</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 62\u001b[39m\n\u001b[32m     51\u001b[39m trainer = Trainer(\n\u001b[32m     52\u001b[39m     model=model,\n\u001b[32m     53\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     58\u001b[39m     compute_metrics=compute_metrics\n\u001b[32m     59\u001b[39m )\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[32m     65\u001b[39m eval_results = trainer.evaluate()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emile\\anaconda3\\envs\\lab1_env\\Lib\\site-packages\\transformers\\trainer.py:2238\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2236\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2237\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2238\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2239\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2241\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2242\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2243\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emile\\anaconda3\\envs\\lab1_env\\Lib\\site-packages\\transformers\\trainer.py:2587\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2581\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m   2582\u001b[39m     tr_loss_step = \u001b[38;5;28mself\u001b[39m.training_step(model, inputs, num_items_in_batch)\n\u001b[32m   2584\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2585\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2586\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m-> \u001b[39m\u001b[32m2587\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   2588\u001b[39m ):\n\u001b[32m   2589\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2590\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n\u001b[32m   2591\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Evita che Hugging Face faccia l'init automatico\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "# ✅ Login + init manuale\n",
    "wandb.login()\n",
    "wandb.init(\n",
    "    project=\"Transformers\",         # Nome progetto scelto da te\n",
    "    name=\"DistilBERT_FineTuned\"     # Nome run scelto da te\n",
    ")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average='weighted'\n",
    "    )\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "# Training args — niente report_to=\"wandb\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True,\n",
    "    report_to=[],                  # <-- disattiva logger HF\n",
    "    run_name=\"DistilBERT_FineTuned\"  # Nome run locale (solo per HF logs)\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate\n",
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)\n",
    "\n",
    "# ✅ Chiudi run W&B\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223dec70",
   "metadata": {},
   "source": [
    "### **SVM Vs Fine-tuned Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b4baed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#SVM baseline \n",
    "svm_test_acc = test_acc  \n",
    "\n",
    "#Fine-tuned DistilBERT\n",
    "ft_test_acc = eval_results['eval_accuracy']  # dai risultati trainer.evaluate()\n",
    "\n",
    "# Plot comparison\n",
    "model_names = [\"SVM (feature extractor)\", \"DistilBERT Fine-tuned\"]\n",
    "accuracies = [svm_test_acc, ft_test_acc]\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar(model_names, accuracies, color=[\"yellow\", \"blue\"])\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel(\"Accuracy on test set\")\n",
    "plt.title(\"Comparison: SVM vs Fine-tuned DistilBERT\")\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8376de-8554-4a13-aac3-59257f3eb3fd",
   "metadata": {},
   "source": [
    "-----\n",
    "### Exercise 3: Choose at Least One\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b55cf4d-e64b-47fc-b8d5-37288b72d90d",
   "metadata": {},
   "source": [
    "#### Exercise 3.1: Efficient Fine-tuning for Sentiment Analysis (easy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f183856-1111-4fe9-81f1-691fe7c1b706",
   "metadata": {},
   "source": [
    "In Exercise 2 we fine-tuned the *entire* Distilbert model on Rotten Tomatoes. This is expensive, even for a small model. Find an *efficient* way to fine-tune Distilbert on the Rotten Tomatoes dataset (or some other dataset).\n",
    "\n",
    "**Hint**: You could check out the [HuggingFace PEFT library](https://huggingface.co/docs/peft/en/index) for some state-of-the-art approaches that should \"just work\". How else might you go about making fine-tuning more efficient without having to change your training pipeline from above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea6bca5-9b36-424e-898c-52c0777eae5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emile\\AppData\\Local\\Temp\\ipykernel_1784\\2906275123.py:45: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='135' max='134' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [134/134 1:01:31, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.500400</td>\n",
       "      <td>0.404229</td>\n",
       "      <td>0.818949</td>\n",
       "      <td>0.819085</td>\n",
       "      <td>0.818949</td>\n",
       "      <td>0.818930</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "'method' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 55\u001b[39m\n\u001b[32m     45\u001b[39m trainer = Trainer(\n\u001b[32m     46\u001b[39m     model=model,\n\u001b[32m     47\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     51\u001b[39m     compute_metrics=compute_metrics\n\u001b[32m     52\u001b[39m )\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[32m     58\u001b[39m eval_results = trainer.evaluate()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emile\\anaconda3\\envs\\lab1_env\\Lib\\site-packages\\transformers\\trainer.py:2238\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2236\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2237\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2238\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2239\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2241\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2242\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2243\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emile\\anaconda3\\envs\\lab1_env\\Lib\\site-packages\\transformers\\trainer.py:2728\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2725\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[32m   2726\u001b[39m         smp.barrier()\n\u001b[32m-> \u001b[39m\u001b[32m2728\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_best_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2730\u001b[39m \u001b[38;5;66;03m# add remaining tr_loss\u001b[39;00m\n\u001b[32m   2731\u001b[39m \u001b[38;5;28mself\u001b[39m._total_loss_scalar += tr_loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emile\\anaconda3\\envs\\lab1_env\\Lib\\site-packages\\transformers\\trainer.py:3007\u001b[39m, in \u001b[36mTrainer._load_best_model\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   3002\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mhasattr\u001b[39m(model, \u001b[33m\"\u001b[39m\u001b[33mactive_adapter\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[33m\"\u001b[39m\u001b[33mactive_adapters\u001b[39m\u001b[33m\"\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\n\u001b[32m   3003\u001b[39m     model, \u001b[33m\"\u001b[39m\u001b[33mload_adapter\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3004\u001b[39m ):\n\u001b[32m   3005\u001b[39m     \u001b[38;5;66;03m# For BC for older PEFT versions\u001b[39;00m\n\u001b[32m   3006\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[33m\"\u001b[39m\u001b[33mactive_adapters\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m3007\u001b[39m         active_adapter = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mactive_adapters\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   3008\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(model.active_adapters) > \u001b[32m1\u001b[39m:\n\u001b[32m   3009\u001b[39m             logger.warning(\u001b[33m\"\u001b[39m\u001b[33mDetected multiple active adapters, will only consider the first one\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: 'method' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function to compute metrics during evaluation\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1) \n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average='weighted'\n",
    "    )\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "# Apply LoRA for parameter-efficient fine-tuning\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                   # low-rank dimension\n",
    "    lora_alpha=16,         # scaling factor\n",
    "    target_modules=[\"q_lin\", \"v_lin\"],  # which weights to adapt\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=,\n",
    "    learning_rate=2e-4,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"st3eps\",\n",
    "    load_best_model_at_end=False,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    fp16=True  # mixed precision training\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate\n",
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab1_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
